{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DE_CONSUMPTION',\n",
       " 'DE_FR_EXCHANGE',\n",
       " 'DE_NET_EXPORT',\n",
       " 'DE_GAS',\n",
       " 'DE_COAL',\n",
       " 'DE_HYDRO',\n",
       " 'DE_NUCLEAR',\n",
       " 'DE_SOLAR',\n",
       " 'DE_WINDPOW',\n",
       " 'DE_LIGNITE',\n",
       " 'DE_RESIDUAL_LOAD',\n",
       " 'GAS_RET',\n",
       " 'COAL_RET',\n",
       " 'CARBON_RET']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "datasets=\"/home/ymachta/Challenges_and_fun/Qube/Datasets/\"\n",
    "df = pd.read_csv(datasets+'X_imputed_DE.csv').drop(['ID',\"DAY_ID\",\"DE_RAIN\",\"DE_WIND\",\"DE_TEMP\"], axis=1)\n",
    "labels=pd.read_csv(datasets+'Y_imputed_DE.csv').drop('ID', axis=1)\n",
    "labels[\"Rank_2\"]=labels[\"Rank\"]**2\n",
    "\n",
    "# df_FR = pd.read_csv(datasets+'X_imputed_FR.csv').drop(['ID',\"DAY_ID\"], axis=1)\n",
    "df_FR = pd.read_csv(datasets+'X_imputed_FR.csv').drop(['ID',\"DAY_ID\",\"FR_DE_EXCHANGE\",\"FR_NET_EXPORT\",\"FR_RAIN\",\"FR_WIND\",\"FR_TEMP\"], axis=1)\n",
    "labels_FR=pd.read_csv(datasets+'Y_imputed_FR.csv').drop('ID', axis=1)\n",
    "\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powers added\n",
      "16369 colunms to add\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16369 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16369/16369 [00:53<00:00, 307.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_uplet added\n",
      "54\n"
     ]
    }
   ],
   "source": [
    "from utils.Brute_force_feature_engineering import *\n",
    "\n",
    "base_columns=list(df.columns)\n",
    "df_copy,corr=main_brute(df,base_columns,labels,n_uplet=20,thresh=0.25)\n",
    "print(len(corr))\n",
    "df_copy2=df_copy[list(corr.index)+base_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powers added\n",
      "2024 colunms to add\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2024/2024 [00:03<00:00, 529.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_uplet added\n",
      "_x_GAS_RET_x_FR_WINDPOW                                                                           0.153600\n",
      "_x_CARBON_RET_x_GAS_RET                                                                           0.150299\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_GAS                                               0.142039\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_SOLAR_x_FR_CONSUMPTION                                              -0.132993\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_SOLAR_x_FR_NUCLEAR_x_FR_HYDRO_x_FR_GAS                              -0.133298\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_SOLAR_x_FR_NUCLEAR_x_FR_HYDRO_x_FR_CONSUMPTION   -0.133704\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_SOLAR_x_FR_HYDRO_x_FR_GAS_x_FR_CONSUMPTION       -0.133785\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_SOLAR_x_FR_HYDRO_x_FR_GAS                                           -0.135605\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_SOLAR_x_FR_NUCLEAR_x_FR_GAS_x_FR_CONSUMPTION                        -0.136024\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_SOLAR_x_FR_HYDRO_x_FR_CONSUMPTION                -0.136252\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_SOLAR_x_FR_HYDRO_x_FR_GAS                        -0.136515\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_SOLAR_x_FR_NUCLEAR_x_FR_CONSUMPTION              -0.136658\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_SOLAR                                            -0.137215\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_SOLAR_x_FR_HYDRO_x_FR_GAS_x_FR_CONSUMPTION                          -0.137238\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_SOLAR_x_FR_NUCLEAR_x_FR_GAS                      -0.138189\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_SOLAR_x_FR_HYDRO_x_FR_CONSUMPTION                                   -0.140783\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_SOLAR_x_FR_GAS_x_FR_CONSUMPTION                  -0.142222\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_SOLAR_x_FR_HYDRO                                 -0.142504\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_SOLAR_x_FR_GAS                                                      -0.144380\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_WINDPOW_x_FR_NUCLEAR                                                -0.152689\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_RESIDUAL_LOAD_x_FR_WINDPOW                                          -0.180726\n",
      "_x_CARBON_RET_x_GAS_RET_x_FR_WINDPOW_x_FR_GAS                                                    -0.181472\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "base_columns_FR=list(df_FR.columns)\n",
    "df_copy_FR,corr_FR=main_brute(df_FR,base_columns_FR,labels_FR,n_uplet=9,thresh=0.13)\n",
    "print(corr_FR)\n",
    "df_copy2_FR=df_copy_FR[list(corr_FR.index)+base_columns_FR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_mask = df_copy.isin([np.inf])\n",
    "df_copy.loc[inf_mask.any(axis=1), inf_mask.any(axis=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_clean_DE = df\n",
    "Y_train_clean_DE = labels\n",
    "X_train_clean_FR = df_FR\n",
    "Y_train_clean_FR = labels_FR\n",
    "X_train_clean_FR, X_test_FR, Y_train_clean_FR, y_test_FR = train_test_split(X_train_clean_FR, Y_train_clean_FR, test_size=0.25)\n",
    "X_train_clean_DE, X_test_DE, Y_train_clean_DE, y_test_DE = train_test_split(X_train_clean_DE, Y_train_clean_DE, test_size=0.25)\n",
    "\n",
    "# print (output_train[:,1])\n",
    "def metric_train(output,df):\n",
    "    term=output[0]\n",
    "    if not isinstance(term, np.ndarray):\n",
    "        return  100 *spearmanr(output, df[\"TARGET\"]).correlation\n",
    "    else:\n",
    "        return  [100 *spearmanr(output[:,i], df[\"TARGET\"]).correlation for i in range(len(term)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_stats(df,labels,regressor,columns=['TARGET','Rank'],iterations=20,depth=None,group=False):\n",
    "    valid_list=[]\n",
    "    for i in range(iterations):\n",
    "        X_train_clean_DE, X_test_DE, Y_train_clean_DE, y_test_DE = train_test_split(df, labels, test_size=0.25)\n",
    "        if group: \n",
    "            Y_train_clean_DE['Rank_group']=pd.qcut(Y_train_clean_DE.Rank,30).cat.codes\n",
    "            if columns is None:\n",
    "                columns= 'Rank_group'\n",
    "            elif 'Rank_group'  not in columns: \n",
    "                if isinstance(columns,list):\n",
    "                    columns=columns+['Rank_group']\n",
    "                else:\n",
    "                    columns=[columns]+['Rank_group']\n",
    "        if depth is None:\n",
    "            DF = regressor()\n",
    "        else:            \n",
    "            DF = regressor(max_depth=depth)\n",
    "        #   Fit the ridge regressor\n",
    "\n",
    "        DF.fit(X_train_clean_DE, Y_train_clean_DE[columns])\n",
    "        output_train = DF.predict(X_test_DE)\n",
    "        # print(output_train)\n",
    "        validation=metric_train(output_train,y_test_DE)\n",
    "        \n",
    "        valid_list+=[validation]\n",
    "    \n",
    "    if isinstance(columns,list):\n",
    "        for i,column in enumerate(columns):\n",
    "            print( f\"average {column} score for \", sum([row[i] for row in valid_list])/len(valid_list) )\n",
    "    else:\n",
    "        print( f\"average {columns} score for \", sum(valid_list)/len(valid_list) )\n",
    "    \n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average TARGET score for  32.352674136333746\n",
      "average Rank score for  32.84419846055667\n",
      "\n",
      "\n",
      "average Rank score for  34.93084012878229\n",
      "\n",
      "\n",
      "average TARGET score for  26.659680594798015\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "average_stats(df_copy2,labels,RandomForestRegressor,depth=3)\n",
    "average_stats(df_copy2,labels,RandomForestRegressor,columns='Rank',depth=3)\n",
    "average_stats(df_copy2,labels,RandomForestRegressor,columns='TARGET',depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average TARGET score for  32.75692439453148\n",
      "average Rank score for  33.262689952599224\n",
      "\n",
      "\n",
      "average Rank score for  34.76429066110044\n",
      "\n",
      "\n",
      "average TARGET score for  25.265252729714547\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_stats(df,labels,RandomForestRegressor,depth=3)\n",
    "average_stats(df,labels,RandomForestRegressor,columns='Rank',depth=3)\n",
    "average_stats(df,labels,RandomForestRegressor,columns='TARGET',depth=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average Rank_group score for  36.30638250694482\n",
      "\n",
      "\n",
      "average Rank_group score for  30.68978835793998\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_stats(df,labels,RandomForestRegressor,columns=None,depth=3,group=True)\n",
    "average_stats(df_copy2,labels,RandomForestRegressor,columns=None,depth=3,group=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average TARGET score for  17.524418953781517\n",
      "average Rank score for  19.002513666718052\n",
      "\n",
      "\n",
      "average Rank score for  18.585382918594313\n",
      "\n",
      "\n",
      "average TARGET score for  8.441262153374547\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_stats(df_FR,labels_FR,RandomForestRegressor,depth=3)\n",
    "average_stats(df_FR,labels_FR,RandomForestRegressor,columns='Rank',depth=3)\n",
    "average_stats(df_FR,labels_FR,RandomForestRegressor,columns='TARGET',depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average TARGET score for  19.429634761550854\n",
      "average Rank score for  20.560434314123267\n",
      "\n",
      "\n",
      "average Rank score for  20.69651055626914\n",
      "\n",
      "\n",
      "average TARGET score for  14.259310752491492\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_stats(df_copy2_FR,labels_FR,RandomForestRegressor,depth=3)\n",
    "average_stats(df_copy2_FR,labels_FR,RandomForestRegressor,columns='Rank',depth=3)\n",
    "average_stats(df_copy2_FR,labels_FR,RandomForestRegressor,columns='TARGET',depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average Rank_group score for  18.46192922678766\n",
      "\n",
      "\n",
      "average Rank_group score for  17.67332174097308\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "average_stats(df_copy2_FR,labels_FR,RandomForestRegressor,columns=None,depth=3,group=True)\n",
    "average_stats(df_FR,labels_FR,RandomForestRegressor,columns=None,depth=3,group=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average TARGET score for  10.874659238669448\n",
      "average Rank score for  30.935364068064494\n",
      "\n",
      "\n",
      "average Rank score for  29.5422527335191\n",
      "\n",
      "\n",
      "average TARGET score for  11.008569212319385\n",
      "\n",
      "\n",
      "average Rank_group score for  29.886952490934064\n",
      "\n",
      "\n",
      "average TARGET score for  19.432688024490634\n",
      "average Rank score for  37.99639649742855\n",
      "\n",
      "\n",
      "average Rank score for  37.43823405406133\n",
      "\n",
      "\n",
      "average TARGET score for  18.25836124093441\n",
      "\n",
      "\n",
      "average Rank_group score for  37.61988403737143\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mord import OrdinalRidge,LAD\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    average_stats(df_copy2,labels,OrdinalRidge,iterations=100)\n",
    "    average_stats(df_copy2,labels,OrdinalRidge,columns='Rank',iterations=100)\n",
    "    average_stats(df_copy2,labels,OrdinalRidge,columns='TARGET',iterations=100)\n",
    "    average_stats(df_copy2,labels,OrdinalRidge,columns=None, group=True,iterations=100)\n",
    "    \n",
    "    \n",
    "    average_stats(df,labels,OrdinalRidge,iterations=100)\n",
    "    average_stats(df,labels,OrdinalRidge,columns='Rank',iterations=100)\n",
    "    average_stats(df,labels,OrdinalRidge,columns='TARGET',iterations=100)\n",
    "    average_stats(df,labels,OrdinalRidge,columns=None, group=True,iterations=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average TARGET score for  nan\n",
      "average Rank score for  16.681366495657585\n",
      "\n",
      "\n",
      "average Rank score for  15.245328766098751\n",
      "\n",
      "\n",
      "average TARGET score for  3.483115463774554\n",
      "\n",
      "\n",
      "average Rank_group score for  16.927773299304082\n",
      "\n",
      "\n",
      "average TARGET score for  nan\n",
      "average Rank score for  18.802887219049854\n",
      "\n",
      "\n",
      "average Rank score for  19.980412204254016\n",
      "\n",
      "\n",
      "average TARGET score for  nan\n",
      "\n",
      "\n",
      "average Rank_group score for  17.812501983223473\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mord import OrdinalRidge,LAD\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    average_stats(df_copy2_FR,labels_FR,OrdinalRidge,iterations=100)\n",
    "    average_stats(df_copy2_FR,labels_FR,OrdinalRidge,columns='Rank',iterations=100)\n",
    "    average_stats(df_copy2_FR,labels_FR,OrdinalRidge,columns='TARGET',iterations=100)\n",
    "    average_stats(df_copy2_FR,labels_FR,OrdinalRidge,columns=None, group=True,iterations=100)\n",
    "    \n",
    "    \n",
    "    average_stats(df_FR,labels_FR,OrdinalRidge,iterations=100)\n",
    "    average_stats(df_FR,labels_FR,OrdinalRidge,columns='Rank',iterations=100)\n",
    "    average_stats(df_FR,labels_FR,OrdinalRidge,columns='TARGET',iterations=100)\n",
    "    average_stats(df_FR,labels_FR,OrdinalRidge,columns=None, group=True,iterations=100)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_clean_DE['Rank_group']=pd.qcut(Y_train_clean_DE.Rank,31).cat.codes\n",
    "\n",
    "Y_train_clean_FR['Rank_group']=pd.qcut(Y_train_clean_FR.Rank,10).cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "583     3\n",
       "267     9\n",
       "333    19\n",
       "92      3\n",
       "173    30\n",
       "       ..\n",
       "324    26\n",
       "221    18\n",
       "132     1\n",
       "165     6\n",
       "355    28\n",
       "Name: Rank_group, Length: 447, dtype: int8"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_clean_DE['Rank_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_ranking as tfr\n",
    "# Ratings data.\n",
    "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
    "# Features of all the available movies.\n",
    "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
    "\n",
    "# Select the basic features.\n",
    "ratings = ratings.map(lambda x: {\n",
    "    \"movie_title\": x[\"movie_title\"],\n",
    "    \"user_id\": x[\"user_id\"],\n",
    "    \"user_rating\": x[\"user_rating\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.map(lambda x: x[\"movie_title\"])\n",
    "users = ratings.map(lambda x: x[\"user_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    mask_token=None)\n",
    "user_ids_vocabulary.adapt(users.batch(1000))\n",
    "\n",
    "movie_titles_vocabulary = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    mask_token=None)\n",
    "movie_titles_vocabulary.adapt(movies.batch(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_func = lambda x: user_ids_vocabulary(x[\"user_id\"])\n",
    "reduce_func = lambda key, dataset: dataset.batch(100)\n",
    "ds_train = ratings.group_by_window(\n",
    "    key_func=key_func, reduce_func=reduce_func, window_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of movie_title: (100,)\n",
      "Example values of movie_title: [b'Man Who Would Be King, The (1975)' b'Silence of the Lambs, The (1991)'\n",
      " b'Next Karate Kid, The (1994)' b'2001: A Space Odyssey (1968)'\n",
      " b'Usual Suspects, The (1995)']\n",
      "\n",
      "Shape of user_id: (100,)\n",
      "Example values of user_id: [b'405' b'405' b'405' b'405' b'405']\n",
      "\n",
      "Shape of user_rating: (100,)\n",
      "Example values of user_rating: [1. 4. 1. 5. 5.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in ds_train.take(1):\n",
    "  for key, value in x.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "    print(f\"Example values of {key}: {value[:5].numpy()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _features_and_labels(\n",
    "    x: Dict[str, tf.Tensor]) -> Tuple[Dict[str, tf.Tensor], tf.Tensor]:\n",
    "  labels = x.pop(\"user_rating\")\n",
    "  return x, labels\n",
    "\n",
    "\n",
    "ds_train2 = ds_train.map(_features_and_labels)\n",
    "\n",
    "ds_train2 = ds_train2.apply(\n",
    "    tf.data.experimental.dense_to_ragged_batch(batch_size=32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of movie_title: (32, None)\n",
      "Example values of movie_title: [[b'Man Who Would Be King, The (1975)'\n",
      "  b'Silence of the Lambs, The (1991)' b'Next Karate Kid, The (1994)']\n",
      " [b'Flower of My Secret, The (Flor de mi secreto, La) (1995)'\n",
      "  b'Little Princess, The (1939)' b'Time to Kill, A (1996)']\n",
      " [b'Kundun (1997)' b'Scream (1996)' b'Power 98 (1995)']]\n",
      "\n",
      "Shape of user_id: (32, None)\n",
      "Example values of user_id: [[b'405' b'405' b'405']\n",
      " [b'655' b'655' b'655']\n",
      " [b'13' b'13' b'13']]\n",
      "\n",
      "Shape of label: (32, None)\n",
      "Example values of label: [[1. 4. 1.]\n",
      " [3. 3. 3.]\n",
      " [5. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "for x, label in ds_train2.take(1):\n",
    "  for key, value in x.items():\n",
    "    print(f\"Shape of {key}: {value.shape}\")\n",
    "    print(f\"Example values of {key}: {value[:3, :3].numpy()}\")\n",
    "    print()\n",
    "  print(f\"Shape of label: {label.shape}\")\n",
    "  print(f\"Example values of label: {label[:3, :3].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensRankingModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, user_vocab, movie_vocab):\n",
    "    super().__init__()\n",
    "\n",
    "    # Set up user and movie vocabulary and embedding.\n",
    "    self.user_vocab = user_vocab\n",
    "    self.movie_vocab = movie_vocab\n",
    "    self.user_embed = tf.keras.layers.Embedding(user_vocab.vocabulary_size(),\n",
    "                                                64)\n",
    "    self.movie_embed = tf.keras.layers.Embedding(movie_vocab.vocabulary_size(),\n",
    "                                                 64)\n",
    "\n",
    "  def call(self, features: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "    # Define how the ranking scores are computed: \n",
    "    # Take the dot-product of the user embeddings with the movie embeddings.\n",
    "\n",
    "    user_embeddings = self.user_embed(self.user_vocab(features[\"user_id\"]))\n",
    "    movie_embeddings = self.movie_embed(\n",
    "        self.movie_vocab(features[\"movie_title\"]))\n",
    "    print(user_embeddings * movie_embeddings)\n",
    "    return tf.reduce_sum(user_embeddings * movie_embeddings, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ranking model, trained with a ranking loss and evaluated with\n",
    "# ranking metrics.\n",
    "model = MovieLensRankingModel(user_ids_vocabulary, movie_titles_vocabulary)\n",
    "optimizer = tf.keras.optimizers.Adagrad(0.5)\n",
    "loss = tfr.keras.losses.get(\n",
    "    loss=tfr.keras.losses.RankingLossKey.SOFTMAX_LOSS, ragged=True)\n",
    "eval_metrics = [\n",
    "    tfr.keras.metrics.get(key=\"ndcg\", name=\"metric/ndcg\", ragged=True),\n",
    "    tfr.keras.metrics.get(key=\"mrr\", name=\"metric/mrr\", ragged=True)\n",
    "]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[39m.\u001b[39;49mfit(ds_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nnunet/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/nnunet/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "model.fit(ds_train, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: Invalid reduction dimension 2 for input with 2 dimensions. for '{{node movie_lens_ranking_model/Sum}} = Sum[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](movie_lens_ranking_model/mul, movie_lens_ranking_model/Sum/reduction_indices)' with input shapes: [?,64], [] and with computed input tensors: input[1] = <2>.\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnunet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
